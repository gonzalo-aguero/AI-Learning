{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97f332f6",
   "metadata": {},
   "source": [
    "# Calling the Llama3 API with HuggingFace\n",
    "In this notebook, we will explore how to use the Hugging Face `InferenceClient` to interact with the Llama 3 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a45f2ab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gonzalo90fa/utn/2025 - CUARTO/Beca I+D/AI_Python_Env/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class='markdown-body'><p>Here's a step-by-step guide to creating an AI agent using Llama 3.1 on Langchain:</p>\n",
       "<p><strong>Prerequisites:</strong></p>\n",
       "<ul>\n",
       "<li>Install Langchain CLI using <code>pip install langchain</code></li>\n",
       "<li>Set up a Llama 3.1 API key</li>\n",
       "</ul>\n",
       "<p><strong>Create an AI Agent:</strong></p>\n",
       "<ol>\n",
       "<li><strong>Initialize Langchain</strong>: Run <code>langchain init</code> to set up your project.</li>\n",
       "<li><strong>Import Llama 3.1</strong>: Add <code>import llama</code> to your project's <code>main.py</code> file.</li>\n",
       "<li><strong>Create a Llama Client</strong>: Create a <code>llama_client</code> instance using your API key: <code>client = llama.Client(api_key=\"YOUR_API_KEY\")</code></li>\n",
       "<li><strong>Define a Question</strong>: Define a question as a string: <code>question = \"What is the capital of France?\"</code></li>\n",
       "<li><strong>Run the Agent</strong>: Use the <code>client</code> to generate a response: <code>response = client.generate(question)</code></li>\n",
       "<li><strong>Print the Response</strong>: Print the generated response: <code>print(response)</code></li>\n",
       "</ol>\n",
       "<p><strong>Run the Agent:</strong></p>\n",
       "<p>Run <code>python main.py</code> to execute your AI agent. You should see the response printed to the console.</p>\n",
       "<p>That's it! You've created a basic AI agent using Llama 3.1 on Langchain.</p></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import InferenceClient\n",
    "import markdown\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def display_markdown(text):\n",
    "    html = markdown.markdown(text, extensions=['fenced_code', 'codehilite'])\n",
    "    html = f\"<div class='markdown-body'>{html}</div>\"\n",
    "    display(HTML(html))\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "access_token = os.getenv(\"HUGGING_FACE_HUB_TOKEN\")\n",
    "\n",
    "# Initialize InferenceClient\n",
    "try:\n",
    "    client = InferenceClient(\n",
    "        provider=\"fireworks-ai\",\n",
    "        api_key=access_token,\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing InferenceClient: {e}\")\n",
    "    client = None\n",
    "\n",
    "# Define system message and prompt\n",
    "system_message = \"You are an experienced AI Engineer who provides simple instructions on creating AI agents using Langchain. Keep your responses concise and focused on practical steps. Avoid unnecessary details and jargon.\"\n",
    "user_message = \"Teach me how to create an AI agent using Llama 3.1.\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": user_message},\n",
    "]\n",
    "\n",
    "\n",
    "# Make the API call and display the response\n",
    "if client:\n",
    "    try:\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "            messages=messages,\n",
    "        )\n",
    "        response_content = completion.choices[0].message.content\n",
    "        html = markdown.markdown(response_content, extensions=['fenced_code', 'codehilite'])\n",
    "        display(HTML(f\"<div class='markdown-body'>{html}</div>\"))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during API call: {e}\")\n",
    "else:\n",
    "    print(\"InferenceClient not initialized, cannot make API call.\")     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
