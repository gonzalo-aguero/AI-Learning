{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97f332f6",
   "metadata": {},
   "source": [
    "# Calling the Llama3 API with HuggingFace\n",
    "In this notebook, we will explore how to use the Hugging Face `InferenceClient` to interact with the Llama 3 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a45f2ab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/gonzalo90fa/HDD disk/AI_Python_Env/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during API call: 404 Client Error: Not Found for url: https://router.huggingface.co/fireworks-ai/inference/v1/chat/completions (Request ID: Root=1-692bd755-5a1baa294d83434a2807d2d6;b238009d-368a-4697-8f74-af01b2d1453f)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import InferenceClient\n",
    "import markdown\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def display_markdown(text):\n",
    "    html = markdown.markdown(text, extensions=['fenced_code', 'codehilite'])\n",
    "    html = f\"<div class='markdown-body'>{html}</div>\"\n",
    "    display(HTML(html))\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "access_token = os.getenv(\"HUGGING_FACE_HUB_TOKEN\")\n",
    "\n",
    "# Initialize InferenceClient\n",
    "try:\n",
    "    client = InferenceClient(\n",
    "        provider=\"fireworks-ai\",\n",
    "        api_key=access_token,\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing InferenceClient: {e}\")\n",
    "    client = None\n",
    "\n",
    "# Define system message and prompt\n",
    "system_message = \"You are an experienced AI Engineer who provides simple instructions on creating AI agents using Langchain. Keep your responses concise and focused on practical steps. Avoid unnecessary details and jargon.\"\n",
    "user_message = \"Teach me how to create an AI agent using Llama 3.1.\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": user_message},\n",
    "]\n",
    "\n",
    "\n",
    "# Make the API call and display the response\n",
    "if client:\n",
    "    try:\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "            messages=messages,\n",
    "        )\n",
    "        response_content = completion.choices[0].message.content\n",
    "        html = markdown.markdown(response_content, extensions=['fenced_code', 'codehilite'])\n",
    "        display(HTML(f\"<div class='markdown-body'>{html}</div>\"))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during API call: {e}\")\n",
    "else:\n",
    "    print(\"InferenceClient not initialized, cannot make API call.\")     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
