{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494f4f9c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'output_dir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TrainingArguments\n\u001b[32m      3\u001b[39m training_args = TrainingArguments(\n\u001b[32m      4\u001b[39m \n\u001b[32m      5\u001b[39m   \u001b[38;5;66;03m# Learning rate\u001b[39;00m\n\u001b[32m      6\u001b[39m   learning_rate=\u001b[32m1.0e-5\u001b[39m,\n\u001b[32m      7\u001b[39m \n\u001b[32m      8\u001b[39m   \u001b[38;5;66;03m# Number of training epochs\u001b[39;00m\n\u001b[32m      9\u001b[39m   num_train_epochs=\u001b[32m1\u001b[39m,\n\u001b[32m     10\u001b[39m \n\u001b[32m     11\u001b[39m   \u001b[38;5;66;03m# Max steps to train for (each step is a batch of data)\u001b[39;00m\n\u001b[32m     12\u001b[39m   \u001b[38;5;66;03m# Overrides num_train_epochs, if not -1\u001b[39;00m\n\u001b[32m     13\u001b[39m   max_steps=\u001b[32m100\u001b[39m,\n\u001b[32m     14\u001b[39m \n\u001b[32m     15\u001b[39m   \u001b[38;5;66;03m# Batch size for training\u001b[39;00m\n\u001b[32m     16\u001b[39m   per_device_train_batch_size=\u001b[32m1\u001b[39m,\n\u001b[32m     17\u001b[39m \n\u001b[32m     18\u001b[39m   \u001b[38;5;66;03m# Directory to save model checkpoints\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m   output_dir=\u001b[43moutput_dir\u001b[49m,\n\u001b[32m     20\u001b[39m \n\u001b[32m     21\u001b[39m   \u001b[38;5;66;03m# Other arguments\u001b[39;00m\n\u001b[32m     22\u001b[39m   overwrite_output_dir=\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;66;03m# Overwrite the content of the output directory\u001b[39;00m\n\u001b[32m     23\u001b[39m   disable_tqdm=\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;66;03m# Disable progress bars\u001b[39;00m\n\u001b[32m     24\u001b[39m   eval_steps=\u001b[32m120\u001b[39m, \u001b[38;5;66;03m# Number of update steps between two evaluations\u001b[39;00m\n\u001b[32m     25\u001b[39m   save_steps=\u001b[32m120\u001b[39m, \u001b[38;5;66;03m# After # steps model is saved\u001b[39;00m\n\u001b[32m     26\u001b[39m   warmup_steps=\u001b[32m1\u001b[39m, \u001b[38;5;66;03m# Number of warmup steps for learning rate scheduler\u001b[39;00m\n\u001b[32m     27\u001b[39m   per_device_eval_batch_size=\u001b[32m1\u001b[39m, \u001b[38;5;66;03m# Batch size for evaluation\u001b[39;00m\n\u001b[32m     28\u001b[39m   \u001b[38;5;66;03m# evaluation_strategy=\"steps\",\u001b[39;00m\n\u001b[32m     29\u001b[39m   logging_strategy=\u001b[33m\"\u001b[39m\u001b[33msteps\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     30\u001b[39m   logging_steps=\u001b[32m1\u001b[39m,\n\u001b[32m     31\u001b[39m   optim=\u001b[33m\"\u001b[39m\u001b[33madafactor\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     32\u001b[39m   gradient_accumulation_steps = \u001b[32m4\u001b[39m,\n\u001b[32m     33\u001b[39m   gradient_checkpointing=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     34\u001b[39m \n\u001b[32m     35\u001b[39m   \u001b[38;5;66;03m# Parameters for early stopping\u001b[39;00m\n\u001b[32m     36\u001b[39m   \u001b[38;5;66;03m# load_best_model_at_end=True,\u001b[39;00m\n\u001b[32m     37\u001b[39m   save_total_limit=\u001b[32m1\u001b[39m,\n\u001b[32m     38\u001b[39m   metric_for_best_model=\u001b[33m\"\u001b[39m\u001b[33meval_loss\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     39\u001b[39m   greater_is_better=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     40\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'output_dir' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "\n",
    "  # Learning rate\n",
    "  learning_rate=1.0e-5,\n",
    "\n",
    "  # Number of training epochs\n",
    "  num_train_epochs=1,\n",
    "\n",
    "  # Max steps to train for (each step is a batch of data)\n",
    "  # Overrides num_train_epochs, if not -1\n",
    "  max_steps=max_steps,\n",
    "\n",
    "  # Batch size for training\n",
    "  per_device_train_batch_size=1,\n",
    "\n",
    "  # Directory to save model checkpoints\n",
    "  output_dir=output_dir,\n",
    "\n",
    "  # Other arguments\n",
    "  overwrite_output_dir=False, # Overwrite the content of the output directory\n",
    "  disable_tqdm=False, # Disable progress bars\n",
    "  eval_steps=120, # Number of update steps between two evaluations\n",
    "  save_steps=120, # After # steps model is saved\n",
    "  warmup_steps=1, # Number of warmup steps for learning rate scheduler\n",
    "  per_device_eval_batch_size=1, # Batch size for evaluation\n",
    "  # evaluation_strategy=\"steps\",\n",
    "  logging_strategy=\"steps\",\n",
    "  logging_steps=1,\n",
    "  optim=\"adafactor\",\n",
    "  gradient_accumulation_steps = 4,\n",
    "  gradient_checkpointing=False,\n",
    "\n",
    "  # Parameters for early stopping\n",
    "  # load_best_model_at_end=True,\n",
    "  save_total_limit=1,\n",
    "  metric_for_best_model=\"eval_loss\",\n",
    "  greater_is_better=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b735a0f0",
   "metadata": {},
   "source": [
    "## EXPLANATION \n",
    "\n",
    "Here is a brief explanation of each argument in the TrainingArguments configuration:\n",
    "\n",
    "learning_rate=1.0e-5: Specifies the initial learning rate for the optimizer, controlling how much the model weights are updated during training.\n",
    "\n",
    "num_train_epochs=1: Sets the number of complete passes through the training dataset. Each epoch processes all training data once.\n",
    "\n",
    "max_steps=max_steps: Limits the total number of training steps (batches). If set, it overrides num_train_epochs.\n",
    "\n",
    "per_device_train_batch_size=1: Defines the batch size for training on each device (e.g., GPU or CPU).\n",
    "\n",
    "output_dir=output_dir: Specifies the directory where model checkpoints and logs will be saved.\n",
    "\n",
    "overwrite_output_dir=False: Prevents overwriting the contents of the output directory if it already exists.\n",
    "\n",
    "disable_tqdm=False: Enables progress bars during training for better visibility of progress.\n",
    "\n",
    "eval_steps=120: Sets the number of training steps between evaluations on the validation dataset.\n",
    "\n",
    "save_steps=120: Determines how often (in steps) the model is saved during training.\n",
    "\n",
    "warmup_steps=1: Specifies the number of steps for a learning rate warm-up, gradually increasing the learning rate at the start of training.\n",
    "\n",
    "per_device_eval_batch_size=1: Defines the batch size for evaluation on each device.\n",
    "\n",
    "logging_strategy=\"steps\": Configures logging to occur at regular intervals of steps.\n",
    "\n",
    "logging_steps=1: Sets the frequency (in steps) for logging training metrics.\n",
    "\n",
    "optim=\"adafactor\": Chooses the Adafactor optimizer, which is memory-efficient and suitable for large models.\n",
    "\n",
    "gradient_accumulation_steps=4: Accumulates gradients over multiple steps before performing a weight update, effectively increasing the batch size.\n",
    "\n",
    "gradient_checkpointing=False: Disables gradient checkpointing, which reduces memory usage at the cost of additional computation.\n",
    "\n",
    "save_total_limit=1: Limits the number of saved checkpoints to retain only the most recent one.\n",
    "\n",
    "metric_for_best_model=\"eval_loss\": Specifies the metric to determine the best model during training (in this case, the evaluation loss).\n",
    "\n",
    "greater_is_better=False: Indicates that a lower value of the metric (e.g., loss) is better.\n",
    "\n",
    "These arguments collectively define the training process, including optimization, checkpointing, and evaluation strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b76c28",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "1. **How to choose a value for `per_device_train_batch_size`?**  \n",
    "   - The batch size depends on the available GPU/CPU memory. Larger batch sizes can improve training stability and speed but require more memory. Start with a small value (e.g., 1 or 2) and increase it until you reach the memory limit of your hardware. If memory is limited, consider using gradient accumulation to simulate larger batch sizes.\n",
    "\n",
    "2. **`eval_steps`, what happens in each evaluation?**  \n",
    "   - At each evaluation step, the model is evaluated on the validation dataset. This involves:\n",
    "     - Running the model in evaluation mode (disabling gradient updates).\n",
    "     - Calculating the loss and other metrics (e.g., accuracy) on the validation dataset.\n",
    "     - Logging the evaluation results for monitoring training progress.\n",
    "     - Optionally saving the model if it performs better than previous checkpoints.\n",
    "\n",
    "3. **How to choose a value for `per_device_eval_batch_size`?**  \n",
    "   - Similar to `per_device_train_batch_size`, this depends on the available memory. However, evaluation does not require gradient computation, so you can often use a larger batch size for evaluation than for training. Experiment with the largest value that fits in memory.\n",
    "\n",
    "4. **Which optimizer can I choose for `optim`?**  \n",
    "   - Common optimizers include:\n",
    "     - `\"adamw\"`: A widely used optimizer for transformer models, combining Adam with weight decay.\n",
    "     - `\"adafactor\"`: A memory-efficient optimizer, especially useful for large models.\n",
    "     - `\"sgd\"`: Stochastic Gradient Descent, suitable for simpler models or specific use cases.\n",
    "     - `\"adam\"`: The standard Adam optimizer, though `\"adamw\"` is generally preferred for transformers.\n",
    "\n",
    "5. **What is gradient accumulation for, and how does it work?**  \n",
    "   - Gradient accumulation allows you to simulate a larger batch size by splitting it across multiple smaller steps. Instead of updating the model weights after every batch, gradients are accumulated over several steps (`gradient_accumulation_steps`) and then used to update the weights. This is useful when memory constraints prevent using a large batch size directly. For example, with `gradient_accumulation_steps=4` and `per_device_train_batch_size=1`, the effective batch size is 4."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
